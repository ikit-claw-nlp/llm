{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0b49f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from libs.TokenEmbedding import TokenEmbedding\n",
    "from libs.CorpusDataset import CorpusDataset\n",
    "from libs.MHA import MultiHeadAttention\n",
    "from transformers import AutoTokenizer\n",
    "import glob\n",
    "torch.set_default_device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "761d003e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"ikit-claw-nlp/toy-llm\")\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": tokenizer.vocab_size,\n",
    "    \"pad_idx\": tokenizer.convert_tokens_to_ids(\"<pad>\"),\n",
    "    \"context_length\": 1024, #max context length\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd7f884",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, embed_dim, eps=1e-5):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.eps = eps\n",
    "        self.scale = nn.Parameter(torch.ones(embed_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(embed_dim))\n",
    "    def forward(self, x):\n",
    "        # x shape [batch_size, seq_len, model_dim]\n",
    "        # var shape [batch_size, seq_len, 1]\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        # mean shape [batch_size, seq_len, 1]\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        # use eps to avoid divided by 0\n",
    "        # x shape [batch_size, seq_len, model_dim]\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        self.scale * norm_x + self.shift\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb000a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GELU, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(\n",
    "                torch.Tensor([2.0 / torch.pi])).to(x.device) * (x + 0.044715 * torch.pow(x, 3)) \n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0dfc133",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(FeedForward, self).__init__()\n",
    "        emb_dim = cfg['emb_dim']\n",
    "        self.layers = nn.Sequential(\n",
    "            [\n",
    "                nn.Linear(emb_dim, 4 * emb_dim),\n",
    "                GELU(),\n",
    "                nn.linear(4 * emb_dim, emb_dim)\n",
    "            ]\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ecfe5a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        assert cfg['emb_dim'] % cfg['n_heads'] == 0, \"Embedding Dim must be integer multiple of the n_heads!\"\n",
    "        head_dim = int(cfg['emb_dim'] / cfg['n_heads'])\n",
    "        self.mha_layer = MultiHeadAttention(cfg['n_heads'], head_dim, cfg['emb_dim'],\n",
    "                                             cfg['context_length'], cfg['drop_rate'],\n",
    "                                             use_qkv_bias=cfg['qkv_bias'],use_mask=True)\n",
    "        self.before_mha_norm = LayerNorm(cfg['emb_dim'])\n",
    "        self.after_mha_norm = LayerNorm(cfg['emb_dim'])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.dropout_after_mha = nn.Dropout(cfg['drop_rate'])\n",
    "        self.dropout_after_ff = nn.Dropout(cfg['drop_rate'])\n",
    "    def forward(self, x):\n",
    "        raw_input = x\n",
    "        x = self.before_mha_norm(x)\n",
    "        x = self.mha_layer(x, x, x)\n",
    "        x = self.dropout_after_mha(x)\n",
    "        # Residual connection.\n",
    "        x = x + raw_input\n",
    "        raw_input = x\n",
    "        x = self.after_mha_norm(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.dropout_after_ff(x)\n",
    "        return x + raw_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea8efff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(DummyTransformerBlock, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc46d20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(DummyGPTModel, self).__init__()\n",
    "        self.token_embeddings = TokenEmbedding(vocab_size=cfg[\"vocab_size\"],\n",
    "                            pad_idx = cfg[\"pad_idx\"],\n",
    "                            seq_length=cfg[\"context_length\"],\n",
    "                            d_model=cfg[\"emb_dim\"],\n",
    "                            dropout=cfg[\"drop_rate\"]\n",
    "        )\n",
    "        self.transformers = nn.Sequential(\n",
    "            * [DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        self.final_norm = DummyLayeNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "    def forward(self, x):\n",
    "        embedding_x = self.token_embeddings(x)\n",
    "        return self.out_head(embedding_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "292e21da",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_files = glob.glob(\"data/text/*.txt\")\n",
    "corpus_dataset = CorpusDataset(text_files, GPT_CONFIG_124M[\"context_length\"], 1, tokenizer)\n",
    "dataloader = torch.utils.data.DataLoader(corpus_dataset, batch_size = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69bfe8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the dataset data/text/article_1-1000.txt into memory...\n",
      "Converting the dataset to token ids...\n",
      "Conversion Complete. torch.Size([7540024]) Tokens in the corpus.\n"
     ]
    }
   ],
   "source": [
    "x, y = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa66ccf",
   "metadata": {},
   "source": [
    "LayerNorm unit_testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f2d2851",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings = TokenEmbedding(vocab_size=GPT_CONFIG_124M[\"vocab_size\"],\n",
    "                            pad_idx = GPT_CONFIG_124M[\"pad_idx\"],\n",
    "                            seq_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "                            d_model=GPT_CONFIG_124M[\"emb_dim\"],\n",
    "                            dropout=GPT_CONFIG_124M[\"drop_rate\"])\n",
    "x_embedding = token_embeddings(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f611249",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LayeNorm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m transformer = \u001b[43mTransformerBlock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGPT_CONFIG_124M\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m trans_x_embed = transformer(x_embedding)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mTransformerBlock.__init__\u001b[39m\u001b[34m(self, cfg)\u001b[39m\n\u001b[32m      5\u001b[39m head_dim = \u001b[38;5;28mint\u001b[39m(cfg[\u001b[33m'\u001b[39m\u001b[33memb_dim\u001b[39m\u001b[33m'\u001b[39m] / cfg[\u001b[33m'\u001b[39m\u001b[33mn_heads\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      6\u001b[39m \u001b[38;5;28mself\u001b[39m.mha_layer = MultiHeadAttention(cfg[\u001b[33m'\u001b[39m\u001b[33mn_heads\u001b[39m\u001b[33m'\u001b[39m], head_dim, cfg[\u001b[33m'\u001b[39m\u001b[33memb_dim\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m      7\u001b[39m                                      cfg[\u001b[33m'\u001b[39m\u001b[33mcontext_length\u001b[39m\u001b[33m'\u001b[39m], cfg[\u001b[33m'\u001b[39m\u001b[33mdrop_rate\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m      8\u001b[39m                                      use_qkv_bias=cfg[\u001b[33m'\u001b[39m\u001b[33mqkv_bias\u001b[39m\u001b[33m'\u001b[39m],use_mask=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28mself\u001b[39m.before_mha_norm = \u001b[43mLayerNorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43memb_dim\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mself\u001b[39m.after_mha_norm = LayerNorm(cfg[\u001b[33m'\u001b[39m\u001b[33memb_dim\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     11\u001b[39m \u001b[38;5;28mself\u001b[39m.ff = FeedForward(cfg)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mLayerNorm.__init__\u001b[39m\u001b[34m(self, embed_dim, eps)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, embed_dim, eps=\u001b[32m1e-5\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[38;5;28msuper\u001b[39m(\u001b[43mLayeNorm\u001b[49m, \u001b[38;5;28mself\u001b[39m).\u001b[34m__init__\u001b[39m()\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mself\u001b[39m.eps = eps\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mself\u001b[39m.scale = nn.Parameter(torch.ones(embed_dim))\n",
      "\u001b[31mNameError\u001b[39m: name 'LayeNorm' is not defined"
     ]
    }
   ],
   "source": [
    "transformer = TransformerBlock(GPT_CONFIG_124M)\n",
    "trans_x_embed = transformer(x_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff03d17d",
   "metadata": {},
   "source": [
    "LayerNorm unit test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b4d1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_norm = LayerNorm(embed_dim=GPT_CONFIG_124M[\"emb_dim\"])\n",
    "norm_x_embedding = layer_norm(x_embedding)\n",
    "var_x_embedding = x_embedding.var(dim=-1, keepdim=True)\n",
    "mean_x_embedding = x_embedding.mean(dim=-1, keepdim=True)\n",
    "var_norm_x_embedding = norm_x_embedding.var(dim=-1, keepdim=True)\n",
    "mean_norm_x_embedding = norm_x_embedding.mean(dim=-1, keepdim=True)\n",
    "print(var_x_embedding[0,0,:], var_norm_x_embedding[0, 0, :])\n",
    "print(mean_x_embedding[0, 0, :], mean_norm_x_embedding[0, 0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e83269",
   "metadata": {},
   "source": [
    "TransformerBlock unit test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2e1e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TransformerBlock(GPT_CONFIG_124M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c27c2e8",
   "metadata": {},
   "source": [
    "GELU impl. unit test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3416e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gelu = GELU()\n",
    "gelu_x_embedding = gelu(x_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc21acef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "gelu, relu = GELU(), nn.ReLU()\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "y_gelu, y_relu = gelu(x), relu(x)\n",
    "plt.figure(figsize=(8, 3))\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n",
    "    plt.subplot(1, 2, i)\n",
    "    plt.plot(x.cpu(), y.cpu())\n",
    "    plt.title(f\"{label} activation function\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(f\"{label}(x)\")\n",
    "    plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
