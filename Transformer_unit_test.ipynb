{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b49f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from libs.TokenEmbedding import TokenEmbedding\n",
    "from libs.CorpusDataset import CorpusDataset\n",
    "from libs.MHA import MultiHeadAttention\n",
    "from transformers import AutoTokenizer\n",
    "import glob\n",
    "torch.set_default_device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761d003e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"ikit-claw-nlp/toy-llm\")\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": tokenizer.vocab_size,\n",
    "    \"pad_idx\": tokenizer.convert_tokens_to_ids(\"<pad>\"),\n",
    "    \"context_length\": 1024, #max context length\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd7f884",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, embed_dim, eps=1e-5):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.eps = eps\n",
    "        self.scale = nn.Parameter(torch.ones(embed_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(embed_dim))\n",
    "    def forward(self, x):\n",
    "        # x shape [batch_size, seq_len, model_dim]\n",
    "        # var shape [batch_size, seq_len, 1]\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        # mean shape [batch_size, seq_len, 1]\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        # use eps to avoid divided by 0\n",
    "        # x shape [batch_size, seq_len, model_dim]\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        self.scale * norm_x + self.shift\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb000a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GELU, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(\n",
    "                torch.Tensor([2.0 / torch.pi])).to(x.device) * (x + 0.044715 * torch.pow(x, 3)) \n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e0dfc133",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(FeedForward, self).__init__()\n",
    "        emb_dim = cfg['emb_dim']\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(emb_dim, 4 * emb_dim),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * emb_dim, emb_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ecfe5a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        assert cfg['emb_dim'] % cfg['n_heads'] == 0, \"Embedding Dim must be integer multiple of the n_heads!\"\n",
    "        head_dim = int(cfg['emb_dim'] / cfg['n_heads'])\n",
    "        self.mha_layer = MultiHeadAttention(cfg['n_heads'], head_dim, cfg['emb_dim'],\n",
    "                                             cfg['context_length'], cfg['drop_rate'],\n",
    "                                             use_qkv_bias=cfg['qkv_bias'],use_mask=True)\n",
    "        self.before_mha_norm = LayerNorm(cfg['emb_dim'])\n",
    "        self.after_mha_norm = LayerNorm(cfg['emb_dim'])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.drop_residual = nn.Dropout(cfg['drop_rate'])\n",
    "    def forward(self, x):\n",
    "        raw_input = x\n",
    "        x = self.before_mha_norm(x)\n",
    "        x, _ = self.mha_layer(x, x, x)\n",
    "        x = self.drop_residual(x)\n",
    "        # Residual connection.\n",
    "        x = x + raw_input\n",
    "        raw_input = x\n",
    "        x = self.after_mha_norm(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_residual(x)\n",
    "        return x + raw_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea8efff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(DummyTransformerBlock, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc46d20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(DummyGPTModel, self).__init__()\n",
    "        self.token_embeddings = TokenEmbedding(vocab_size=cfg[\"vocab_size\"],\n",
    "                            pad_idx = cfg[\"pad_idx\"],\n",
    "                            seq_length=cfg[\"context_length\"],\n",
    "                            d_model=cfg[\"emb_dim\"],\n",
    "                            dropout=cfg[\"drop_rate\"]\n",
    "        )\n",
    "        self.transformers = nn.Sequential(\n",
    "            * [DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        self.final_norm = DummyLayeNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "    def forward(self, x):\n",
    "        embedding_x = self.token_embeddings(x)\n",
    "        return self.out_head(embedding_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292e21da",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_files = glob.glob(\"data/text/*.txt\")\n",
    "corpus_dataset = CorpusDataset(text_files, GPT_CONFIG_124M[\"context_length\"], 1, tokenizer)\n",
    "dataloader = torch.utils.data.DataLoader(corpus_dataset, batch_size = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bfe8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa66ccf",
   "metadata": {},
   "source": [
    "Transformer block unit_testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2d2851",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings = TokenEmbedding(vocab_size=GPT_CONFIG_124M[\"vocab_size\"],\n",
    "                            pad_idx = GPT_CONFIG_124M[\"pad_idx\"],\n",
    "                            seq_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "                            d_model=GPT_CONFIG_124M[\"emb_dim\"],\n",
    "                            dropout=GPT_CONFIG_124M[\"drop_rate\"])\n",
    "x_embedding = token_embeddings(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8f611249",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TransformerBlock(GPT_CONFIG_124M)\n",
    "trans_x_embed = transformer(x_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff03d17d",
   "metadata": {},
   "source": [
    "LayerNorm unit test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b4d1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_norm = LayerNorm(embed_dim=GPT_CONFIG_124M[\"emb_dim\"])\n",
    "norm_x_embedding = layer_norm(x_embedding)\n",
    "var_x_embedding = x_embedding.var(dim=-1, keepdim=True)\n",
    "mean_x_embedding = x_embedding.mean(dim=-1, keepdim=True)\n",
    "var_norm_x_embedding = norm_x_embedding.var(dim=-1, keepdim=True)\n",
    "mean_norm_x_embedding = norm_x_embedding.mean(dim=-1, keepdim=True)\n",
    "print(var_x_embedding[0,0,:], var_norm_x_embedding[0, 0, :])\n",
    "print(mean_x_embedding[0, 0, :], mean_norm_x_embedding[0, 0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e83269",
   "metadata": {},
   "source": [
    "TransformerBlock unit test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2e1e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TransformerBlock(GPT_CONFIG_124M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f604a308",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = nn.Dropout(GPT_CONFIG_124M['drop_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c27c2e8",
   "metadata": {},
   "source": [
    "GELU impl. unit test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3416e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gelu = GELU()\n",
    "gelu_x_embedding = gelu(x_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc21acef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "gelu, relu = GELU(), nn.ReLU()\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "y_gelu, y_relu = gelu(x), relu(x)\n",
    "plt.figure(figsize=(8, 3))\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n",
    "    plt.subplot(1, 2, i)\n",
    "    plt.plot(x.cpu(), y.cpu())\n",
    "    plt.title(f\"{label} activation function\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(f\"{label}(x)\")\n",
    "    plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
