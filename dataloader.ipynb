{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a71308",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b33bbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer we've trained.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ikit-claw-nlp/toy-llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbf4566",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9696a66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(tokenizer, window_size, step_length):\n",
    "    corpus_files = glob.glob(\"./data/full_text/*.txt\")\n",
    "    for corpus_f in corpus_files:\n",
    "        with open(corpus_f, 'r') as f_handle:\n",
    "            current_corpus = f_handle.read()\n",
    "            tokenized_current_corpus_input_ids = tokenizer.encode(current_corpus)\n",
    "        for idx in range(0, len(tokenized_current_corpus_input_ids) - window_size, step_length):\n",
    "            # Note that, in there we drop the last part of the corpus if it cannot form a full-size window.\n",
    "            # we do not use <pad> to pad the last part of the corpus.\n",
    "            input_ids = tokenized_current_corpus_input_ids[idx : idx + window_size]\n",
    "            output_ids = tokenized_current_corpus_input_ids[idx + 1 : idx + 1 + window_size]\n",
    "            yield input_ids, output_ids\n",
    "            # For those who want to handle the boundary case:\n",
    "            # [a, b, c, d, e, f] => text. Step_size = 2, window_size = 4\n",
    "            # First window: (input) [a, b, c, d] (output) [b, c, d, e] => idx = 0\n",
    "            # second window: (input) [c, d, e, f] (output) [d, e, f, pad] => idx = 2\n",
    "            # third window: (input) [e, f, pad, pad] (output) [f, pad, pad] => idx = 4\n",
    "            # the number of pad = idx + window_size - len(text) for input.\n",
    "            #                   = idx + 1 + window_size - len(text) for output.\n",
    "            # I choose to ignore using <pad> as input in training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3c6126",
   "metadata": {},
   "source": [
    "Test iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758126f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = generate_training_data(tokenizer, 256, 128)\n",
    "for input_tensor, output_tensor in data_iter:\n",
    "    print(len(input_tensor))\n",
    "    print(len(output_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78772117",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
