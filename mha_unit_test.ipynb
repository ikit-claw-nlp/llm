{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a1d8b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch.nn as nn\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a612f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, head_dim, d_model, use_mask=False):\n",
    "        # head_dim = d_k\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.W_Q = nn.Linear(d_model, n_heads * head_dim, bias=False)\n",
    "        self.W_K = nn.Linear(d_model, n_heads * head_dim, bias=False)\n",
    "        self.W_V = nn.Linear(d_model, n_heads * head_dim, bias=False)\n",
    "        self.W_O = nn.Linear(n_heads * head_dim, d_model, bias=False)\n",
    "        self.use_mask = use_mask\n",
    "    def forward(self, Q, K, V):\n",
    "        # Q size: from [batch_size, seq_len, d_model]\n",
    "        Q = self.W_Q(Q) # => [batch_size, seq_len, n_heads * n_dim]\n",
    "        K = self.W_K(K) # => [batch_size, seq_len, n_heads * n_dim]\n",
    "        V = self.W_V(V) # => [batch_size, seq_len, n_heads * n_dim]\n",
    "        sim_mat = torch.matmul(Q, K.transpose(-2, -1)) # sim_mat => [batch_size, seq_len, seq_len]\n",
    "        sim_mat = sim_mat / torch.sqrt(torch.Tensor([self.head_dim]))\n",
    "        if self.use_mask:\n",
    "            seq_len = Q.shape[-2]\n",
    "            mask_mat = torch.ones( (seq_len, seq_len) )\n",
    "            # row elements above the 1 offset of the main diagonal set to 1. Others 0.\n",
    "            mask_mat = torch.triu(mask_mat, diagonal=1)\n",
    "            mask_mat = mask_mat.masked_fill(mask_mat.bool(), -torch.inf)\n",
    "            sim_mat = sim_mat + mask_mat\n",
    "        # context_mat = [batch_size, seq_len, seq_len]\n",
    "        context_mat = nn.functional.softmax(sim_mat, dim=-1)\n",
    "        result_mat = torch.matmul(context_mat, V)\n",
    "        result_mat = self.W_O(result_mat)\n",
    "        return result_mat\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e093c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"ikit-claw-nlp/toy-llm\")\n",
    "d_model = 512\n",
    "n_seq_len = 256\n",
    "n_batch_size = 10\n",
    "EmbeddingLayer = nn.Embedding(\n",
    "    num_embeddings=tokenizer.vocab_size,\n",
    "    embedding_dim = d_model,\n",
    "    padding_idx=tokenizer.convert_tokens_to_ids(\"<pad>\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbeb9672",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = EmbeddingLayer(torch.randint(low=0, high=tokenizer.vocab_size, size=(n_batch_size, n_seq_len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ad09ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_k = 64\n",
    "n_heads = 12\n",
    "print(\"Embedding Size\", embedding.shape)\n",
    "W_Q = nn.Linear(d_model, n_heads * d_k, bias=False)\n",
    "W_K = nn.Linear(d_model, n_heads * d_k, bias=False)\n",
    "W_V = nn.Linear(d_model, n_heads * d_k, bias=False)\n",
    "W_O = nn.Linear(n_heads * d_k, d_model, bias=False)\n",
    "Q = W_Q(embedding)\n",
    "K = W_K(embedding).transpose(-1, -2)\n",
    "V = W_V(embedding)\n",
    "print(\"Q, K, V shapes\")\n",
    "print(Q.shape, K.shape, V.shape)\n",
    "similar_mat = torch.matmul(Q, K)\n",
    "# Masking\n",
    "mask_matrix = torch.triu(torch.ones(n_seq_len, n_seq_len), diagonal=1)\n",
    "mask_matrix = mask_matrix.masked_fill(mask_matrix.bool(), -torch.inf)\n",
    "print(\"Masking Matrix\")\n",
    "print(mask_matrix)\n",
    "print(mask_matrix.shape)\n",
    "similar_mat = similar_mat + mask_matrix\n",
    "softmax_mat = torch.softmax(similar_mat / torch.sqrt(torch.Tensor([d_model])), dim=-1)\n",
    "print(\"Softmax Matrix Shape\")\n",
    "print(softmax_mat.shape)\n",
    "print(\"Contextual Matrix\")\n",
    "result_mat = torch.matmul(softmax_mat, V)\n",
    "print(result_mat.shape)\n",
    "print(\"Final Output\")\n",
    "result_mat = W_O(result_mat)\n",
    "print(result_mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "286429dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "test_tensor = torch.rand(size=(10, 256, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0a59daab",
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiHeadAttention(\n",
    "    n_heads = 8,\n",
    "    head_dim = 64,\n",
    "    d_model = 100,\n",
    "    use_mask= True \n",
    ")\n",
    "test_mat = mha(test_tensor,test_tensor,test_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "64b705d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 256, 256])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mat.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
