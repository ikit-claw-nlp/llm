{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a1d8b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch.nn as nn\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a612f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, head_dim, d_model, seq_len, dropout =0.0, use_mask=False):\n",
    "        # head_dim = d_k\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.seq_len = seq_len\n",
    "        self.head_dim = head_dim\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.W_Q = nn.Linear(d_model, n_heads * head_dim, bias=False)\n",
    "        self.W_K = nn.Linear(d_model, n_heads * head_dim, bias=False)\n",
    "        self.W_V = nn.Linear(d_model, n_heads * head_dim, bias=False)\n",
    "        # The book added a linear projection of the final output!\n",
    "        self.output_proj = nn.Linear(n_heads * head_dim, n_heads * head_dim)\n",
    "        # The book doesn't coding W_O in the attention paper!\n",
    "        # self.W_O = nn.Linear(n_heads * head_dim, d_model, bias=False)\n",
    "        self.use_mask = use_mask\n",
    "        if self.use_mask:\n",
    "            self.register_buffer(\n",
    "                \"mask\",\n",
    "                torch.triu(\n",
    "                    torch.ones(self.seq_len, self.seq_len),\n",
    "                    diagonal=1\n",
    "                )\n",
    "            )\n",
    "    def forward(self, Q, K, V):\n",
    "        batch_size, token_len, d_model = Q.shape\n",
    "        # Q size: from [batch_size, seq_len, d_model]\n",
    "        Q = self.W_Q(Q) # => [batch_size, seq_len, n_heads * n_dim]\n",
    "        K = self.W_K(K) # => [batch_size, seq_len, n_heads * n_dim]\n",
    "        V = self.W_V(V) # => [batch_size, seq_len, n_heads * n_dim]\n",
    "\n",
    "        # Modify the view of each tensor.\n",
    "        Q = Q.view(batch_size, token_len, self.n_heads, self.head_dim)\n",
    "        K = K.view(batch_size, token_len, self.n_heads, self.head_dim)\n",
    "        V = V.view(batch_size, token_len, self.n_heads, self.head_dim)\n",
    "\n",
    "        # Q, K, V => [batch_size, num_head, token_len, head_dim]\n",
    "        Q = Q.transpose(2, 1)\n",
    "        K = K.transpose(2, 1)\n",
    "        V = V.transpose(2, 1)\n",
    "        # attention_weights => [batch_size, num_head, token_len, token_len]\n",
    "        attention_weights = torch.matmul(Q, K.transpose(-2, -1)) \n",
    "        attention_weights = attention_weights / Q.shape[-1]**0.5\n",
    "        if self.use_mask:\n",
    "            mask = self.mask.bool()[:token_len, :token_len]\n",
    "            attention_weights.masked_fill_(mask, -torch.inf)\n",
    "        # attention_weights = [batch_size, num_head, token_len, token_len]\n",
    "        attention_weights = nn.functional.softmax(attention_weights, dim=-1)\n",
    "        # drop out some attention_weights.\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        attention_score = attention_weights\n",
    "        # context_vec = [batch_size, num_head, token_len, head_dim]\n",
    "        context_vec = torch.matmul(attention_weights, V)\n",
    "        # context_vec = [batch_size, token_len, num_head, head_dim]\n",
    "        context_vec = context_vec.transpose(2, 1)\n",
    "        context_vec = context_vec.contiguous().view(\n",
    "            batch_size, token_len, self.n_heads * self.head_dim\n",
    "        )\n",
    "        context_vec = self.output_proj(context_vec)\n",
    "        return context_vec, attention_score\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70e093c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"ikit-claw-nlp/toy-llm\")\n",
    "d_model = 512\n",
    "n_seq_len = 256\n",
    "n_batch_size = 10\n",
    "TokenEmbeddingLayer = nn.Embedding(\n",
    "    num_embeddings=tokenizer.vocab_size,\n",
    "    embedding_dim = d_model,\n",
    "    padding_idx=tokenizer.convert_tokens_to_ids(\"<pad>\")\n",
    ")\n",
    "PosEmbeddingLayer = nn.Embedding(\n",
    "    num_embeddings = n_seq_len,\n",
    "    embedding_dim = d_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbeb9672",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = TokenEmbeddingLayer(torch.randint(low=0, high=tokenizer.vocab_size, size=(n_batch_size, n_seq_len)))\n",
    "pos_embedding = PosEmbeddingLayer(torch.arange(n_seq_len))\n",
    "embedding = embedding + pos_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "286429dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "test_tensor = torch.rand(size=(n_batch_size, n_seq_len, d_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a59daab",
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiHeadAttention(\n",
    "    n_heads = 8,\n",
    "    head_dim = 64,\n",
    "    d_model = d_model,\n",
    "    seq_len = n_seq_len,\n",
    "    dropout=0.5,\n",
    "    use_mask= True \n",
    ")\n",
    "test_mat, _ = mha(test_tensor,test_tensor,test_tensor)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
