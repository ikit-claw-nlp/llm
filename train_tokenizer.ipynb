{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19d1868e",
   "metadata": {},
   "source": [
    "Reference:\n",
    "\n",
    "- https://zenn.dev/syoyo/articles/8647ae42a3be63\n",
    "- https://github.com/huggingface/tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0ca2c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import SentencePieceBPETokenizer\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87630ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_files = glob.glob(\"./data/text/*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942f217c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SentencePieceBPETokenizer()\n",
    "# [PAD] has ID 0, [UNK] has id 1, etc.\n",
    "# cf. https://huggingface.co/docs/tokenizers/v0.20.3/en/quicktour?code=python#training-the-tokenizer, cited as follows.\n",
    "# The order in which you write the special tokens list matters: here \"[UNK]\" will get the ID 0, \"[CLS]\" will get the ID 1 and so forth.\n",
    "special_tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"[EOA]\"]\n",
    "tokenizer.train(\n",
    "    files = corpus_files,\n",
    "    vocab_size = 30000,\n",
    "    min_frequency = 5,\n",
    "    special_tokens = special_tokens,\n",
    "    show_progress = True # This is not working in Jupyter notebook\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
