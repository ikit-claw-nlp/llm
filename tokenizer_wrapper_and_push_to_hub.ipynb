{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83268ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "from tokenizers import SentencePieceBPETokenizer\n",
    "from tokenizers.implementations import SentencePieceUnigramTokenizer\n",
    "from transformers import PreTrainedTokenizerFast, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c725e0c5",
   "metadata": {},
   "source": [
    "Convert `sentencepiece` Tokenizer to PreTrainedTokenizerFast model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "071ba70e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tokenizer/ikit-llm-spm/tokenizer_config.json',\n",
       " 'tokenizer/ikit-llm-spm/special_tokens_map.json',\n",
       " 'tokenizer/ikit-llm-spm/tokenizer.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_tokenizer = SentencePieceUnigramTokenizer.from_spm(filename='tokenizer/ikit_spm.model')\n",
    "hf_tokenizer.save('tokenizer/hf_ikit_spm.json')\n",
    "special_tokens = [\"<s>\", \"<pad>\", \"</s>\", \"<unk>\"]\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"tokenizer/hf_ikit_spm.json\", special_tokens=special_tokens)\n",
    "tokenizer.bos_token = \"<s>\"\n",
    "tokenizer.pad_token = \"<pad>\"\n",
    "tokenizer.eos_token = \"</s>\"\n",
    "tokenizer.unk_token = \"<unk>\"\n",
    "# save for later!\n",
    "tokenizer.save_pretrained(\"tokenizer/ikit-llm-spm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047cd9cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/ikit-claw-nlp/toy-llm/commit/38839d225b5ff06be0157c5d5b7daf989647edda', commit_message='Upload tokenizer', commit_description='', oid='38839d225b5ff06be0157c5d5b7daf989647edda', pr_url=None, repo_url=RepoUrl('https://huggingface.co/ikit-claw-nlp/toy-llm', endpoint='https://huggingface.co', repo_type='model', repo_id='ikit-claw-nlp/toy-llm'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"\")\n",
    "tokenizer.push_to_hub(repo_id='ikit-claw-nlp/toy-llm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "130cc872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 22181, 24893, 8205, 4811, 4528, 437, 70, 3623, 12, 56, 305, 1718, 603, 66, 7, 45, 12, 19446, 979, 24893, 268, 45, 12, 19446, 979, 24893, 9, 35, 12895, 15419, 75, 3437, 4789, 4502, 14, 1265, 638, 6, 3437, 4789, 4502, 420, 10, 33267, 7, 0, 3, 2] \n",
      " [4, 1, 22181, 24893, 8205, 4811, 4528, 437, 70, 3623, 12, 56, 305, 1718, 603, 66, 7, 45, 12, 19446, 979, 24893, 268, 45, 12, 19446, 979, 24893, 9, 35, 12895, 15419, 75, 3437, 4789, 4502, 14, 1265, 638, 6, 3437, 4789, 4502, 420, 10, 33267, 7, 0, 3, 2]\n",
      "['<s>', '▁', '日本初の', '博士課程', 'のみの', '国立', '大学院', '大学', 'として', '1988', '年', '10', '月に', '開', '学', 'した', '。', '5', '年', '一貫', '制', '博士課程', 'および', '5', '年', '一貫', '制', '博士課程', 'の', '3', '年次', 'に編入', 'する', '博士', '後期', '課程', '(', '一部', 'については', '、', '博士', '後期', '課程', 'のみ', ')', 'を設置している', '。', '<unk>', '<pad>', '</s>'] \n",
      " ['▁', '<s>', '日本初の', '博士課程', 'のみの', '国立', '大学院', '大学', 'として', '1988', '年', '10', '月に', '開', '学', 'した', '。', '5', '年', '一貫', '制', '博士課程', 'および', '5', '年', '一貫', '制', '博士課程', 'の', '3', '年次', 'に編入', 'する', '博士', '後期', '課程', '(', '一部', 'については', '、', '博士', '後期', '課程', 'のみ', ')', 'を設置している', '。', '<unk>', '<pad>', '</s>']\n"
     ]
    }
   ],
   "source": [
    "repo_tokenizer = AutoTokenizer.from_pretrained(\"ikit-claw-nlp/toy-llm\")\n",
    "sentence = '<s>日本初の博士課程のみの国立大学院大学として1988年10月に開学した。5年一貫制博士課程および5年一貫制博士課程の3年次に編入する博士後期課程（一部については、博士後期課程のみ）を設置している。😂<pad></s>'\n",
    "repo_tokenizer_ids = repo_tokenizer.encode(sentence)\n",
    "tokenizer_ids = tokenizer.encode(sentence)\n",
    "print(\n",
    "    repo_tokenizer_ids,\n",
    "    \"\\n\",\n",
    "    tokenizer_ids\n",
    ")\n",
    "print(\n",
    "    repo_tokenizer.convert_ids_to_tokens(repo_tokenizer_ids),\n",
    "    \"\\n\",\n",
    "    tokenizer.convert_ids_to_tokens(tokenizer_ids)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8995f2e",
   "metadata": {},
   "source": [
    "Convert SentencePieceBPETokenizer to PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb43d722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This must be the same list as the special_tokens in the train_tokenizer.py.\n",
    "special_tokens = [\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<cls>\", \"<sep>\", \"<mask>\"]\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"tiny_llm_tokenizer/tokenizer.json\", special_tokens=special_tokens)\n",
    "tokenizer.bos_token = \"<s>\"\n",
    "tokenizer.pad_token = \"<pad>\"\n",
    "tokenizer.eos_token = \"</s>\"\n",
    "tokenizer.unk_token = \"<unk>\"\n",
    "tokenizer.cls_token = \"<cls>\"\n",
    "tokenizer.sep_token = \"<sep>\"\n",
    "tokenizer.mask_token = \"<mask>\"\n",
    "# and save for later!\n",
    "tokenizer.save_pretrained(\"ikit-llm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8dbcd8-b9b6-4564-83bd-f50c90f6b971",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8604de79-9475-4175-8a34-e7d0d6242519",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.push_to_hub(repo_id='ikit-claw-nlp/toy-llm')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
