{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83268ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "from tokenizers import SentencePieceBPETokenizer\n",
    "from tokenizers.implementations import SentencePieceUnigramTokenizer\n",
    "from transformers import PreTrainedTokenizerFast, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c725e0c5",
   "metadata": {},
   "source": [
    "Convert `sentencepiece` Tokenizer to PreTrainedTokenizerFast model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "071ba70e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tokenizer/ikit-llm-spm/tokenizer_config.json',\n",
       " 'tokenizer/ikit-llm-spm/special_tokens_map.json',\n",
       " 'tokenizer/ikit-llm-spm/tokenizer.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_tokenizer = SentencePieceUnigramTokenizer.from_spm(filename='tokenizer/ikit_spm.model')\n",
    "hf_tokenizer.save('tokenizer/hf_ikit_spm.json')\n",
    "special_tokens = [\"<s>\", \"<pad>\", \"</s>\", \"<unk>\"]\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"tokenizer/hf_ikit_spm.json\", special_tokens=special_tokens)\n",
    "tokenizer.bos_token = \"<s>\"\n",
    "tokenizer.pad_token = \"<pad>\"\n",
    "tokenizer.eos_token = \"</s>\"\n",
    "tokenizer.unk_token = \"<unk>\"\n",
    "# save for later!\n",
    "tokenizer.save_pretrained(\"tokenizer/ikit-llm-spm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047cd9cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/ikit-claw-nlp/toy-llm/commit/38839d225b5ff06be0157c5d5b7daf989647edda', commit_message='Upload tokenizer', commit_description='', oid='38839d225b5ff06be0157c5d5b7daf989647edda', pr_url=None, repo_url=RepoUrl('https://huggingface.co/ikit-claw-nlp/toy-llm', endpoint='https://huggingface.co', repo_type='model', repo_id='ikit-claw-nlp/toy-llm'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"\")\n",
    "tokenizer.push_to_hub(repo_id='ikit-claw-nlp/toy-llm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "130cc872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 22181, 24893, 8205, 4811, 4528, 437, 70, 3623, 12, 56, 305, 1718, 603, 66, 7, 45, 12, 19446, 979, 24893, 268, 45, 12, 19446, 979, 24893, 9, 35, 12895, 15419, 75, 3437, 4789, 4502, 14, 1265, 638, 6, 3437, 4789, 4502, 420, 10, 33267, 7, 0, 3, 2] \n",
      " [4, 1, 22181, 24893, 8205, 4811, 4528, 437, 70, 3623, 12, 56, 305, 1718, 603, 66, 7, 45, 12, 19446, 979, 24893, 268, 45, 12, 19446, 979, 24893, 9, 35, 12895, 15419, 75, 3437, 4789, 4502, 14, 1265, 638, 6, 3437, 4789, 4502, 420, 10, 33267, 7, 0, 3, 2]\n",
      "['<s>', 'â–', 'æ—¥æœ¬åˆã®', 'åšå£«èª²ç¨‹', 'ã®ã¿ã®', 'å›½ç«‹', 'å¤§å­¦é™¢', 'å¤§å­¦', 'ã¨ã—ã¦', '1988', 'å¹´', '10', 'æœˆã«', 'é–‹', 'å­¦', 'ã—ãŸ', 'ã€‚', '5', 'å¹´', 'ä¸€è²«', 'åˆ¶', 'åšå£«èª²ç¨‹', 'ãŠã‚ˆã³', '5', 'å¹´', 'ä¸€è²«', 'åˆ¶', 'åšå£«èª²ç¨‹', 'ã®', '3', 'å¹´æ¬¡', 'ã«ç·¨å…¥', 'ã™ã‚‹', 'åšå£«', 'å¾ŒæœŸ', 'èª²ç¨‹', '(', 'ä¸€éƒ¨', 'ã«ã¤ã„ã¦ã¯', 'ã€', 'åšå£«', 'å¾ŒæœŸ', 'èª²ç¨‹', 'ã®ã¿', ')', 'ã‚’è¨­ç½®ã—ã¦ã„ã‚‹', 'ã€‚', '<unk>', '<pad>', '</s>'] \n",
      " ['â–', '<s>', 'æ—¥æœ¬åˆã®', 'åšå£«èª²ç¨‹', 'ã®ã¿ã®', 'å›½ç«‹', 'å¤§å­¦é™¢', 'å¤§å­¦', 'ã¨ã—ã¦', '1988', 'å¹´', '10', 'æœˆã«', 'é–‹', 'å­¦', 'ã—ãŸ', 'ã€‚', '5', 'å¹´', 'ä¸€è²«', 'åˆ¶', 'åšå£«èª²ç¨‹', 'ãŠã‚ˆã³', '5', 'å¹´', 'ä¸€è²«', 'åˆ¶', 'åšå£«èª²ç¨‹', 'ã®', '3', 'å¹´æ¬¡', 'ã«ç·¨å…¥', 'ã™ã‚‹', 'åšå£«', 'å¾ŒæœŸ', 'èª²ç¨‹', '(', 'ä¸€éƒ¨', 'ã«ã¤ã„ã¦ã¯', 'ã€', 'åšå£«', 'å¾ŒæœŸ', 'èª²ç¨‹', 'ã®ã¿', ')', 'ã‚’è¨­ç½®ã—ã¦ã„ã‚‹', 'ã€‚', '<unk>', '<pad>', '</s>']\n"
     ]
    }
   ],
   "source": [
    "repo_tokenizer = AutoTokenizer.from_pretrained(\"ikit-claw-nlp/toy-llm\")\n",
    "sentence = '<s>æ—¥æœ¬åˆã®åšå£«èª²ç¨‹ã®ã¿ã®å›½ç«‹å¤§å­¦é™¢å¤§å­¦ã¨ã—ã¦1988å¹´10æœˆã«é–‹å­¦ã—ãŸã€‚5å¹´ä¸€è²«åˆ¶åšå£«èª²ç¨‹ãŠã‚ˆã³5å¹´ä¸€è²«åˆ¶åšå£«èª²ç¨‹ã®3å¹´æ¬¡ã«ç·¨å…¥ã™ã‚‹åšå£«å¾ŒæœŸèª²ç¨‹ï¼ˆä¸€éƒ¨ã«ã¤ã„ã¦ã¯ã€åšå£«å¾ŒæœŸèª²ç¨‹ã®ã¿ï¼‰ã‚’è¨­ç½®ã—ã¦ã„ã‚‹ã€‚ğŸ˜‚<pad></s>'\n",
    "repo_tokenizer_ids = repo_tokenizer.encode(sentence)\n",
    "tokenizer_ids = tokenizer.encode(sentence)\n",
    "print(\n",
    "    repo_tokenizer_ids,\n",
    "    \"\\n\",\n",
    "    tokenizer_ids\n",
    ")\n",
    "print(\n",
    "    repo_tokenizer.convert_ids_to_tokens(repo_tokenizer_ids),\n",
    "    \"\\n\",\n",
    "    tokenizer.convert_ids_to_tokens(tokenizer_ids)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8995f2e",
   "metadata": {},
   "source": [
    "Convert SentencePieceBPETokenizer to PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb43d722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This must be the same list as the special_tokens in the train_tokenizer.py.\n",
    "special_tokens = [\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<cls>\", \"<sep>\", \"<mask>\"]\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"tiny_llm_tokenizer/tokenizer.json\", special_tokens=special_tokens)\n",
    "tokenizer.bos_token = \"<s>\"\n",
    "tokenizer.pad_token = \"<pad>\"\n",
    "tokenizer.eos_token = \"</s>\"\n",
    "tokenizer.unk_token = \"<unk>\"\n",
    "tokenizer.cls_token = \"<cls>\"\n",
    "tokenizer.sep_token = \"<sep>\"\n",
    "tokenizer.mask_token = \"<mask>\"\n",
    "# and save for later!\n",
    "tokenizer.save_pretrained(\"ikit-llm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8dbcd8-b9b6-4564-83bd-f50c90f6b971",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8604de79-9475-4175-8a34-e7d0d6242519",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.push_to_hub(repo_id='ikit-claw-nlp/toy-llm')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
